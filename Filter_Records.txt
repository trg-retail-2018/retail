from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
from pyspark.sql.functions import *
#import org.apache.spark.sql.SaveMode

# Run script by using:
# spark-submit2 --packages mysql:mysql-connector-java:5.1.39,com.databricks:spark-avro_2.11:4.0.0 initial_load.py

#Main function

def main():

	# Set up spark context 
        sc = SparkContext("local[2]", "NetworkWordCount")
	# Set up sql context
	sqlContext = SQLContext(sc)

	# Create promotion dataframe. Mysqlconnector package is required for the driver
	# Change url to jdbc:mysql://${HOSTNAME}:3306/${DATABASE_NAME}
	# Change user, dbtable and password accordingly 
	promotion_df = sqlContext.read.format("jdbc").options(
	    url="jdbc:mysql://localhost:3306/foodmart",
	    driver = "com.mysql.jdbc.Driver",
	    dbtable = "promotion",
	    user="root",
	    password="cloudera").load()
	
	
	
	


	sales_fact_1997_df = sqlContext.read.format("jdbc").options(
            url="jdbc:mysql://localhost:3306/foodmart",
            driver = "com.mysql.jdbc.Driver",
            dbtable = "sales_fact_1997",
            user="root",
            password="cloudera").load()



	sales_fact_1998_df = sqlContext.read.format("jdbc").options(
            url="jdbc:mysql://localhost:3306/foodmart",
            driver = "com.mysql.jdbc.Driver",
            dbtable = "sales_fact_1998",
            user="root",
            password="cloudera").load()	

	pro_RF_df = sqlContext.read.format("com.databricks.spark.avro").load("/home/cloudera/Shwetha/case_study/raw/promotion/part-00000-4413fa7f-fa67-4b42-b008-8880cde0a6b5-c000.avro")
	pro_df = pro_RF_df.where(col("promotion_id") > 0)
	sales97_RF_df = sqlContext.read.format("com.databricks.spark.avro").load("/home/cloudera/Shwetha/case_study/raw/sales97/part-00000-1883b16c-b9ff-4983-b29b-f76d7171d155-c000.avro")
	sales97_df = sales97_RF_df.where(col("promotion_id") > 0)
	sales98_RF_df = sqlContext.read.format("com.databricks.spark.avro").load("/home/cloudera/Shwetha/case_study/raw/sales98/part-00000-a98ca35a-40c8-4640-9fd3-573f93f1dd29-c000.avro")
	sales98_df = sales98_RF_df.where(col("promotion_id") > 0)
	
	
	sales_DF = sales97_df.unionAll(sales98_df).show()
	final_DF = pro_df.join(sales_DF, pro_df.promotion_id == sales_DF.promotion_id).drop(sales_DF.promotion_id)
	final_DF.write.mode(SaveMode.Append).format("parquet").save("/home/cloudera/Shwetha/case_study/parquet")

	# Just a print statement to see if the dataframe transferred sucessfully
	#print promotion_df.show()
	#print promotion_readfile_df.show()
	#print sales_fact_1997_df.show()
	#print sales_fact_1998_df.show()
	#print pro_df.show()
	#print sales97_df.show()
	#print sales98_df.show()
	#print sales_DF.show()
	#print final_DF.show()

	promotion_df.write.format("com.databricks.spark.avro").save("file:///home/cloudera/Shwetha/case_study/raw/promotion")
	sales_fact_1997_df.write.format("com.databricks.spark.avro").save("file:///home/cloudera/Shwetha/case_study/raw/sales97")
	sales_fact_1998_df.write.format("com.databricks.spark.avro").save("file:///home/cloudera/Shwetha/case_study/raw/sales98")
	




# Runs the script
if __name__ == "__main__":
	main()